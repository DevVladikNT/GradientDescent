# GradientDescent

*test*

## О проекте

Что-то о проекте

## Как работает

### Рассмотрим задачу линейной регрессии.

Пусть даны объекты $x_i$ и соответствующие им $y_i = k x_i + b + \epsilon$, где $\epsilon$ - шум в данных.
Нужно построить модель $f(x_i) = a_0 + a_1 x_i$, которая будет лучше всего описывать наши данные (то есть $f(x_i) \rightarrow y_i$).

Коэффициенты $a_0$ и $a_1$ представим в виде вектора:
```math
A = \begin{pmatrix}
a_0\\
a_1
\end{pmatrix},
```
$x_i$ и $y_i$ соответственно:
```math
X = \begin{pmatrix}
1 & x_1\\
1 & x_2\\
... & ...\\
1 & x_n
\end{pmatrix},\quad
Y = \begin{pmatrix}
y_1\\
y_2\\
...\\
y_n
\end{pmatrix}.
```
В таком случае $Y^* = XA, Y^* \rightarrow Y$.

### Функция потерь и градиент

Существуют различные функции потерь (loss), которые показывают насколько "далеко" наши посчитанные значения от тех, которые должны быть.
Например, можно смотреть на сумму квадратов разностей между реальными и полученными значениями $loss = \sum (y_i - y_i^*)^2$.
Если разделить полученное значение на количество элементов $n$, то получим популярную функцию потерь Mean Squared Error (MSE).

Основные метрики в задачах регрессии:
* Mean Absolute Error (MAE) $= \frac{1}{n} \ast \sum |y_i - y_i^*|$
* Mean Absolute Percentage Error (MAPE) $= \frac{1}{n} \ast \sum |\frac{y_i - y_i^*}{y_i}|$
* Mean Squared Error (MSE) $= \frac{1}{n} \ast \sum (y_i - y_i^*)^2$
* Root Mean Squared Error (RMSE) $= \sqrt{\frac{1}{n} \ast \sum (y_i - y_i^*)^2}$

Теперь вспомним, что градиент функции в точке - это вектор, показывающий направление наибольшего возрастания функции в данной точке.
Суть метода в том, чтобы считать градиент **функции потерь** и идти в обратном направлении (в сторону антиградиента), что позволит приблизиться к точке минимума функции потерь.
А теперь вспомним что этого мы и хотим: найти такие коэффициенты $A$, при которых разница между реальными $Y$ и полученными значениями $Y^*$ была минимальной.

*Когда я разбирался с методом градиентного спуска, сновной проблемой было понимание того, что градиент мы считаем для **функции потерь**.
При этом в интернете чаще всего можно увидеть картинки, где нарисованы круги (иногда разными цветами) и стрелочка идет в центр самого маленького.*



```math
\begin{pmatrix}
1 & 2 & 3\\
a & b & c
\end{pmatrix}
```
