# GradientDescent

*В какой-то момент я переведу текст ниже на английский язык и он будет в этом месте.*

## О проекте

Градиентный спуск... Вы можете спросить меня *"Зачем изобретать велосипед?"*.
Поэтому, для начала, хотелось бы сказать зачем этот проект нужен лично мне. В какой-то момент я решил заняться ML и градиент занимает здесь одно из главных мест.
После первого собеседования в VK, я понял, что не до конца понимаю что это вообще такое и решил разобраться. После небольшого катарсиса я сел пописать код ручками
(чтобы лучше усвоилось) и теперь делюсь результатами. Возможно, у кого-то возникают схожие с моими вопросы; поэтому, надеюсь, моя работа кому-то поможет.

## Как работает

В этом блоке я вкратце опишу суть метода и поясню в какой момент в моей голове произошла нестыковка.

### Задача линейной регрессии.

Пусть даны объекты $x_i$ и соответствующие им значения $y_i = k x_i + b + \epsilon$, где $\epsilon$ - шум в данных.

Нужно построить модель $f(x_i) = a_0 + a_1 x_i$, которая будет лучше всего описывать наши данные (то есть $f(x_i) \rightarrow y_i$).

Коэффициенты $a_0$ и $a_1$ представим в виде вектора:
```math
A = \begin{pmatrix}
a_0\\
a_1
\end{pmatrix},
```
$x_i$ и $y_i$ соответственно:
```math
X = \begin{pmatrix}
1 & x_1\\
1 & x_2\\
... & ...\\
1 & x_n
\end{pmatrix},\quad
Y = \begin{pmatrix}
y_1\\
y_2\\
...\\
y_n
\end{pmatrix}.
```
В таком случае $Y^* = XA, Y^* \rightarrow Y$.

### Функция потерь

Существуют различные функции потерь (loss), которые показывают насколько "далеко" наши посчитанные значения от тех, которые должны быть.
Например, можно смотреть на сумму квадратов разностей между реальными и полученными значениями $loss = \sum (y_i - y_i^*)^2$.
Если разделить полученное значение на количество элементов $n$, то получим популярную функцию потерь Mean Squared Error (MSE).

Основные метрики в задачах регрессии:
* Mean Absolute Error (MAE) $= \frac{1}{n} \ast \sum |y_i - y_i^*|$
* Mean Absolute Percentage Error (MAPE) $= \frac{1}{n} \ast \sum |\frac{y_i - y_i^*}{y_i}|$
* Mean Squared Error (MSE) $= \frac{1}{n} \ast \sum (y_i - y_i^*)^2$
* Root Mean Squared Error (RMSE) $= \sqrt{\frac{1}{n} \ast \sum (y_i - y_i^*)^2}$

### Градиент

Теперь вспомним, что градиент функции в точке - это вектор, показывающий направление наибольшего возрастания функции в данной точке.
Суть метода в том, чтобы считать градиент **функции потерь** и идти в обратном направлении (в сторону антиградиента), что позволит приблизиться к точке минимума функции потерь.
А теперь вспомним что этого мы и хотим: найти такие коэффициенты $A$, при которых разница между реальными $Y$ и полученными значениями $Y^*$ будет минимальной.

Градиент считается как вектор частных производных функции по всем аргументам. В этот момент и возникла нестыковка. Изначально, мне сразу в голову приходила наша функция $f(x_i)$,
коэффициенты которой мы ищем. К тому же, можно визуализировать процесс изменения функции с каждым шагом градиентного спуска, и будет видно, что есть "оптимальное" положение функции, которое мы ищем, к которому наша функция стремится. В голове возник вопрос: может градиент показывает в сторону этого оптимального положения? В этот момент я и потерялся.

А теперь стоит рассказать как дело обстоит на самом деле. Для этого рассмотрим самый простой пример, который может быть.
Допустим, у нас есть одна точка $(x_1, y_1)$ и мы хотим найти такие коэффициенты $a_0$ и $a_1$, чтобы $a_1 x_1 + a_0 = y_1$.
Понятно, что таких функций бесконечно мого, но это сейчас не важно. В таком случае:
```math
A = \begin{pmatrix}
a_0\\
a_1
\end{pmatrix},\quad
X = \begin{pmatrix}
1 & x_1
\end{pmatrix},\quad
Y = \begin{pmatrix}
y_1
\end{pmatrix}.
```
Допустим, мы находимся на каком-то промежуточном шаге и получили коэффициенты $a_0, a_1$. Посчитаем функцию потерь: $loss = (a_1^* x_1 + a_0^* - y_1)^2$.

Теперь поймем, что мы хотим уменьшить $loss$. А для этого будем менять коэффициенты $a_0, a_1$ (параметры модели), так как у нас $x_1, y_1$ уже зафиксированы.

Можно нарисовать плоскость, на осях которой будут значения $a_0, a_1$. Тогда существует прямая $a_0 = y_1 - x_1 a_1$, точки которой, при подстановке в исходную функцию
как коэффициенты, будут давать функции, проходящие через заданную по условию точку $(x_1, y_1)$. А для каждой точки в пространстве параметров модели, не лежащей на такой прямой,
$loss > 0$, и чем дальше точка, тем больше $loss$. То есть над плоскостью, являющейся пространством параметров модели, можно нарисовать поверхность функции потерь.

По сути, мы будем строить градиент **функции потерь** в **пространстве параметров** модели и скускаться постепенно к ее минимуму.
Именно этого понимания мне не хватало раньше.

